{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8346529,"sourceType":"datasetVersion","datasetId":4958257},{"sourceId":8354411,"sourceType":"datasetVersion","datasetId":4952829}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom transformers import default_data_collator\nfrom transformers import HfArgumentParser\nimport pandas as pd\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_name = \"t5-small\"\nconfig = AutoConfig.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config).to(device)\n\n# dataset = load_dataset('findnitai/english-to-hinglish')\n# master = [line['en'] for line in dataset['train']['translation']]\n# master += [line['hi_ng'] for line in dataset['train']['translation']]\n\ndf = pd.read_csv(\"/kaggle/input/engtohing/train_new.txt\", sep='\\t', header=None, names=['eng', 'hing'])\nmaster = df['eng'].tolist()\nmaster+= df['hing'].tolist()\n\n# dataset.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T15:03:01.995733Z","iopub.execute_input":"2024-05-08T15:03:01.996149Z","iopub.status.idle":"2024-05-08T15:03:14.329264Z","shell.execute_reply.started":"2024-05-08T15:03:01.996114Z","shell.execute_reply":"2024-05-08T15:03:14.328407Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72931e261aa4bcf9d709338157b4807"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3237f2ff40c34fc2b31c8020147e581e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b4a3019f37442b5baf9aa07f421adfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2888df8fb1334c68a3d0b09a388e88cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b1252074e347aba6ffb9794648385d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c532ef766cf2490d9f7149b746b4a35f"}},"metadata":{}}]},{"cell_type":"code","source":"print(config)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:03:14.330997Z","iopub.execute_input":"2024-05-08T15:03:14.331452Z","iopub.status.idle":"2024-05-08T15:03:14.338322Z","shell.execute_reply.started":"2024-05-08T15:03:14.331420Z","shell.execute_reply":"2024-05-08T15:03:14.337287Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"T5Config {\n  \"_name_or_path\": \"t5-small\",\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"classifier_dropout\": 0.0,\n  \"d_ff\": 2048,\n  \"d_kv\": 64,\n  \"d_model\": 512,\n  \"decoder_start_token_id\": 0,\n  \"dense_act_fn\": \"relu\",\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"is_gated_act\": false,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"t5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 8,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_max_distance\": 128,\n  \"relative_attention_num_buckets\": 32,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"early_stopping\": true,\n      \"length_penalty\": 2.0,\n      \"max_length\": 200,\n      \"min_length\": 30,\n      \"no_repeat_ngram_size\": 3,\n      \"num_beams\": 4,\n      \"prefix\": \"summarize: \"\n    },\n    \"translation_en_to_de\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to German: \"\n    },\n    \"translation_en_to_fr\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to French: \"\n    },\n    \"translation_en_to_ro\": {\n      \"early_stopping\": true,\n      \"max_length\": 300,\n      \"num_beams\": 4,\n      \"prefix\": \"translate English to Romanian: \"\n    }\n  },\n  \"transformers_version\": \"4.39.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 32128\n}\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(df))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:03:14.339546Z","iopub.execute_input":"2024-05-08T15:03:14.339860Z","iopub.status.idle":"2024-05-08T15:03:14.347848Z","shell.execute_reply.started":"2024-05-08T15:03:14.339808Z","shell.execute_reply":"2024-05-08T15:03:14.346905Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"248330\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset\ndata = []\nfor index, row in df.iterrows():\n    data.append({\n        'en': row['eng'],\n        'hi_ng': row['hing']\n    })\n\n# Create a Dataset object\ndataset = Dataset.from_dict({'translation': data})\n\nprint(dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:03:14.349894Z","iopub.execute_input":"2024-05-08T15:03:14.350239Z","iopub.status.idle":"2024-05-08T15:03:29.516452Z","shell.execute_reply.started":"2024-05-08T15:03:14.350216Z","shell.execute_reply":"2024-05-08T15:03:29.515471Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['translation'],\n    num_rows: 248330\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:03:29.517682Z","iopub.execute_input":"2024-05-08T15:03:29.518007Z","iopub.status.idle":"2024-05-08T15:03:29.527866Z","shell.execute_reply.started":"2024-05-08T15:03:29.517980Z","shell.execute_reply":"2024-05-08T15:03:29.526792Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'translation': {'en': 'Hindi Milaap From Hyderbad', 'hi_ng': 'Hindi Milaap From Hyderbad'}}\n","output_type":"stream"}]},{"cell_type":"code","source":"def gen_training_data():\n    return (master[i : i+500] for i in range(0, len(master), 500))\n\ntokenizer_training_data = gen_training_data()\ntokenizer = tokenizer.train_new_from_iterator(tokenizer_training_data, 32128)\n\ndef preprocess(source_data):\n    inputs = [sample['en'] for sample in source_data[\"translation\"]]\n    targets = [sample['hi_ng'] for sample in source_data[\"translation\"]]\n    inputs = [\"Translate English to Hinglish: \" + inp for inp in inputs]\n    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n    labels[\"input_ids\"] = [[l if l != tokenizer.pad_token_id else -100 for l in label] for label in labels[\"input_ids\"]]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nraw_dataset = {\"train\": dataset}\ntrain_dataset = raw_dataset[\"train\"].map(preprocess, batched=True, remove_columns=\"translation\")\nprint(train_dataset)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T15:03:29.529113Z","iopub.execute_input":"2024-05-08T15:03:29.529389Z","iopub.status.idle":"2024-05-08T15:03:51.519232Z","shell.execute_reply.started":"2024-05-08T15:03:29.529365Z","shell.execute_reply":"2024-05-08T15:03:51.517754Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (master[i : i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m500\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(master), \u001b[38;5;241m500\u001b[39m))\n\u001b[1;32m      4\u001b[0m tokenizer_training_data \u001b[38;5;241m=\u001b[39m gen_training_data()\n\u001b[0;32m----> 5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_new_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer_training_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(source_data):\n\u001b[1;32m      8\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m source_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranslation\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:791\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.train_new_from_iterator\u001b[0;34m(self, text_iterator, vocab_size, length, new_special_tokens, special_tokens_map, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m trainer_class \u001b[38;5;241m=\u001b[39m MODEL_TO_TRAINER_MAPPING[tokenizer_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    790\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainer_class(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 791\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     trained_tokenizer_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tokenizer\u001b[38;5;241m.\u001b[39mto_str())\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(raw_dataset['train']['translation'][:5])","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:03:51.520448Z","iopub.status.idle":"2024-05-08T15:03:51.520799Z","shell.execute_reply.started":"2024-05-08T15:03:51.520631Z","shell.execute_reply":"2024-05-08T15:03:51.520645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = default_data_collator\n\nnum_epochs = 5\ntrainer_args_in = {\n    'output_dir': 'my-t5-hinglish-translator',\n    'overwrite_output_dir': True,\n    'do_train': True,\n    'per_device_train_batch_size': 8,\n    'num_train_epochs': num_epochs,\n    'save_strategy': 'no',\n    'report_to' : []\n}\n\nparser = HfArgumentParser((Seq2SeqTrainingArguments,))\ntraining_args = parser.parse_dict(trainer_args_in)\ntrainer = Seq2SeqTrainer(model=model, args=training_args[0], train_dataset=train_dataset, tokenizer=tokenizer, data_collator=data_collator)\n\ntrain_result = trainer.train(resume_from_checkpoint=None)\ntrainer.save_model()\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"my-t5-hinglish-translator\").to(device)\ninput_text = \"translate English to Hinglish: How is the weather?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids)\nprint(\"Test Output: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T15:03:51.521961Z","iopub.status.idle":"2024-05-08T15:03:51.522271Z","shell.execute_reply.started":"2024-05-08T15:03:51.522118Z","shell.execute_reply":"2024-05-08T15:03:51.522131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# from datasets import load_dataset, Dataset\n# from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n# from transformers import default_data_collator\n# import pandas as pd\n# from transformers import HfArgumentParser\n\n# # Check if GPU is available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # Model configuration\n# model_name = \"t5-small\"\n# config = AutoConfig.from_pretrained(model_name)\n# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n# model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=config).to(device)\n\n# df = pd.read_csv(\"/kaggle/input/engtohing/train_new.txt\", sep='\\t', header=None, names=['eng', 'hing'])\n# master = df['eng'].tolist()\n# master+= df['hing'].tolist()\n\n# data = []\n# for index, row in df.iterrows():\n#     data.append({\n#         'en': row['eng'],\n#         'hi_ng': row['hing']\n#     })\n\n# # Create a Dataset object\n# dataset = Dataset.from_dict({'translation': data})\n\n# # Tokenizer training data\n# def gen_training_data():\n#     return (master[i : i+500] for i in range(0, len(master), 500))\n\n# tokenizer_training_data = gen_training_data()\n# tokenizer = tokenizer.train_new_from_iterator(tokenizer_training_data, 32128)\n\n# # Data preprocessing\n# def preprocess(source_data):\n#     inputs = [sample['en'] for sample in source_data[\"translation\"]]\n#     targets = [sample['hi_ng'] for sample in source_data[\"translation\"]]\n#     inputs = [\"Translate English to Hinglish: \" + inp for inp in inputs]\n#     model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n#     labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n#     labels[\"input_ids\"] = [[l if l != tokenizer.pad_token_id else -100 for l in label] for label in labels[\"input_ids\"]]\n#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n#     return model_inputs\n\n# raw_dataset = {\"train\": dataset}\n# train_dataset = raw_dataset[\"train\"].map(preprocess, batched=True, remove_columns=\"translation\")\n# data_collator = default_data_collator\n\n# # # Training arguments\n# # num_epochs = 3\n# # training_args = Seq2SeqTrainingArguments(\n# #     output_dir='my-t5-hinglish-translator',\n# #     overwrite_output_dir=True,\n# #     do_train=True,\n# #     per_device_train_batch_size=8,\n# #     num_train_epochs=num_epochs,\n# #     'save_strategy': 'no'\n# # )\n\n# num_epochs = 2\n# trainer_args_in = {\n#     'output_dir': 'my-t5-hinglish-translator',\n#     'overwrite_output_dir': True,\n#     'do_train': True,\n#     'per_device_train_batch_size': 8,\n#     'num_train_epochs': num_epochs,\n#     'save_strategy': 'no'\n# }\n\n# parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n# training_args = parser.parse_dict(trainer_args_in)\n# # trainer = Seq2SeqTrainer(model=model, args=training_args[0], train_dataset=train_dataset, tokenizer=tokenizer, data_collator=data_collator)\n\n\n# # Load checkpoint if available\n# checkpoint_path = '/kaggle/input/t5-3epoch/kaggle/working/my-t5-hinglish-translator'  # Update with your checkpoint folder path\n# if checkpoint_path:\n#     trainer = Seq2SeqTrainer(\n#         model=model,\n#         args=training_args[0],\n#         train_dataset=train_dataset,\n#         tokenizer=tokenizer,\n#         data_collator=data_collator\n#     )\n# else:\n#     trainer = Seq2SeqTrainer(\n#         model=model,\n#         args=training_args,\n#         train_dataset=train_dataset,\n#         tokenizer=tokenizer,\n#         data_collator=data_collator\n#     )\n\n# # Continue training\n# trainer.train(resume_from_checkpoint=checkpoint_path)\n# trainer.save_model()\n\n# # Inference\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"my-t5-hinglish-translator\").to(device)\n# input_text = \"translate English to Hinglish: How is the weather?\"\n# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n# outputs = model.generate(input_ids)\n# print(\"Test Output: \" + tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2024-05-07T13:06:16.611227Z","iopub.status.idle":"2024-05-07T13:06:16.611675Z","shell.execute_reply.started":"2024-05-07T13:06:16.611449Z","shell.execute_reply":"2024-05-07T13:06:16.611467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/t5-3epoch/kaggle/working/my-t5-hinglish-translator\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/t5-3epoch/kaggle/working/my-t5-hinglish-translator\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:43:44.672042Z","iopub.execute_input":"2024-05-08T15:43:44.672403Z","iopub.status.idle":"2024-05-08T15:43:45.178909Z","shell.execute_reply.started":"2024-05-08T15:43:44.672376Z","shell.execute_reply":"2024-05-08T15:43:45.178082Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Generate output\ninput_text = \"translate english to hinglish: This fact is based on possibility\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids)\noutput_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Input:\", input_text)\nprint(\"Output:\", output_string)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:46:21.075389Z","iopub.execute_input":"2024-05-08T15:46:21.076119Z","iopub.status.idle":"2024-05-08T15:46:21.183083Z","shell.execute_reply.started":"2024-05-08T15:46:21.076085Z","shell.execute_reply":"2024-05-08T15:46:21.182130Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Input: translate english to hinglish: This fact is based on possibility\nOutput: यह fact possibility पर based है ।\n","output_type":"stream"}]},{"cell_type":"code","source":"import locale\ndef getpreferredencoding(do_setlocale = True):\n    return \"UTF-8\"\nlocale.getpreferredencoding = getpreferredencoding\n!zip -r my-t5-hinglish-translator /kaggle/working/my-t5-hinglish-translator\nfrom IPython.display import FileLink\nFileLink(r'my-t5-hinglish-translator.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:43:48.727469Z","iopub.execute_input":"2024-05-08T15:43:48.728077Z","iopub.status.idle":"2024-05-08T15:43:49.744310Z","shell.execute_reply.started":"2024-05-08T15:43:48.728045Z","shell.execute_reply":"2024-05-08T15:43:49.743207Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/my-t5-hinglish-translator/ (stored 0%)\n  adding: kaggle/working/my-t5-hinglish-translator/runs/ (stored 0%)\n  adding: kaggle/working/my-t5-hinglish-translator/runs/May07_15-25-02_b42c6a7ce932/ (stored 0%)\n  adding: kaggle/working/my-t5-hinglish-translator/runs/May07_15-25-02_b42c6a7ce932/events.out.tfevents.1715095503.b42c6a7ce932.34.0 (deflated 69%)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/my-t5-hinglish-translator.zip","text/html":"<a href='my-t5-hinglish-translator.zip' target='_blank'>my-t5-hinglish-translator.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\ndef translate_text_pytorch(text, model, tokenizer):\n    input_text = \"translate english to hinglish: \" + text\n    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n    outputs = model.generate(input_ids)\n    output_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return output_string\n\n\ndf_test = pd.read_csv('/kaggle/input/engtohing/test_new.txt', names=['en', 'hing'], usecols=['en', 'hing'], sep='\\t')\nen_test = df_test['en']\nhing_test = df_test['hing']\ntexts = list(en_test)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:47:49.184778Z","iopub.execute_input":"2024-05-08T15:47:49.185584Z","iopub.status.idle":"2024-05-08T15:47:49.206095Z","shell.execute_reply.started":"2024-05-08T15:47:49.185548Z","shell.execute_reply":"2024-05-08T15:47:49.205179Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"input_text = \"translate english to hinglish: \"+texts[0]\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\noutputs = model.generate(input_ids)\noutput_string = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(input_text ,output_string)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:47:25.342919Z","iopub.execute_input":"2024-05-08T15:47:25.343324Z","iopub.status.idle":"2024-05-08T15:47:25.468626Z","shell.execute_reply.started":"2024-05-08T15:47:25.343295Z","shell.execute_reply":"2024-05-08T15:47:25.467732Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"translate english to hinglish: This fact is based on possibility यह fact possibility पर based है ।\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntranslated = []\nfor text in tqdm(texts):\n    translated.append(translate_text_pytorch(text, model, tokenizer))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:47:51.825869Z","iopub.execute_input":"2024-05-08T15:47:51.826711Z","iopub.status.idle":"2024-05-08T15:51:04.874264Z","shell.execute_reply.started":"2024-05-08T15:47:51.826677Z","shell.execute_reply":"2024-05-08T15:51:04.873218Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"  0%|          | 0/2000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n100%|██████████| 2000/2000 [03:13<00:00, 10.36it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('translated.txt', 'w', encoding='utf-8') as file:\n    for translate in translated:\n        file.write(translate + '\\n')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T15:51:09.951779Z","iopub.execute_input":"2024-05-08T15:51:09.952388Z","iopub.status.idle":"2024-05-08T15:51:09.959163Z","shell.execute_reply.started":"2024-05-08T15:51:09.952355Z","shell.execute_reply":"2024-05-08T15:51:09.958197Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}